{"content":{"out":{"attributes":{"title":"Simple Linear Regression For Simple Soul","date":"2021-07-05"},"body":"<p>Okay! Linear regression, the basic of the regression analysis. There are surely better implementations of linear regression in some other language that is faster and more intuitive than the one that we will be writing. So why would anyone want another linear regression implementation?. I don&#39;t have an answer to be honest, this is from a project that I am currently working on, and I thought maybe I could share it here, as there lacked a connection between the implementation side and the mathematical side of linear regression.  Here we will be looking at the maths and at the same time implement that maths into code. The codes will be choppy as this is an initial prototype (classic excuse) so sorry about that!!</p>\n<h1 id=\"maths\">Maths?!</h1>\n<p>The <a href=\"https://en.wikipedia.org/wiki/Linear_regression\">wikipedia page</a> for linear regression is the best summary on linear regression. The maths is concrete and clear to understand as it needs only the basic understanding of matrices and linear algebra. Okay! So let&#39;s get into just a bit of maths and then a bit of coding in between.</p>\n<p>Linear regression ,as the name suggests, predicts the relation between a dependent variable $$y$$ based on a single or set of explanatory variables $$x$$ under the assumption that the data is linear.  We know that a linear equation is given as: $$y = mx + c$$ so in a linear regression we are trying to predict the $$m$$ parameter which I prefer to call $$\\theta$$ as it is just the slope. So linear regression boils down to finding the slope of the given data. This is everything that you need to know tbh, other things just naturally follow as we code along. </p>\n<h1 id=\"maths-into-code\">Maths into C<span style=\"font-size: 0.5em;\">ode</span></h1>\n<p>This blog will produce the most basic form of linear regression. Our optimizer will be <a href=\"https://en.wikipedia.org/wiki/Gradient_descent\"> gradient descent method </a> , which is just betting against the gradient until we win or go bust. Quick formula for gradient descent (I love  writing formula in $$\\LaTeX$$, sorry cannot help).</p>\n<p><span style=\"display:table;margin:0 auto;\">$$x_{n+1} = x_n - \\gamma_n \\varDelta F(\\relax{x})$$</span></p>\n<p>Here, $$\\varDelta F\\relax(x)$$ is the gradient of the function $$F \\relax(x)$$, $$\\gamma_n$$ is a often called a step-size. SGD is used all over the place, and it is also the father of other optimization methods like Adam. Enough with this jargon lets get writing.</p>\n<p>Okay! So the first thing that we need is basic amount of matrix algebra. Lets say we want to predict a dependent variable $$y$$ based on explanatory variables $$X$$. Lets say that we have $$p$$ numbers of features that we can predict our results from and there are $$n$$ number of samples. </p>\n<p>So the explanatory matrix ($$X$$) can be given as,</p>\n<span style=\"display:table;margin:0 auto;\">\n$$\nX = {\n\\begin{pmatrix}\nx_{11} & \\cdots & x_{1p} \\\\\nx_{21} & \\cdots & x_{2p} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nx_{n1} & \\cdots & x_{np}\n\\end{pmatrix}\n}\n$$</span>\n\n<p>The size of the matrix $$X$$ is $$n \\times p$$ on wikipedia you will see a extra column at the beginning consisting of all $$1s$$ but we won&#39;t struggle with that here as this is a very simple implementation. In computer terms, $$X$$ is what we feed to the model to predict from or train from depending on the situation. </p>\n<p>Lets define the $$y$$ now. So there can only be one prediction to a series of input features so the shape of prediction(dependent) matrix is $$n \\times 1.$$ In matrix terms,</p>\n<span style=\"display:table;margin:0 auto;\">\n$$\ny = {\n\\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{pmatrix}\n}\n$$\n</span>\n\n<p>This is the labels that we will be sending to the model to learn from and also the prediction we get from the trained model.\nNow the part that we predict, $$\\theta$$. The $$\\theta$$ is of shape $$p \\times 1$$ so that we can multiply it with $$X$$. As for two matrices to be able to multiply with each other the number of rows on second matrix must be equal to number of columns on second matrix.  As a tradition here in this blog lets her is the matrix representation.</p>\n<span style=\"display:table;margin:0 auto;\">\n$$\n\\theta = {\n\\begin{pmatrix}\n\\theta_1 \\\\\n\\theta_2 \\\\\n\\vdots \\\\\n\\theta_p\n\\end{pmatrix}\n}\n$$</span>\n\n<p>In this implementation we will be ignoring the $$\\varepsilon$$ - error variable. Now combining all these things together, our final equation is given as:</p>\n<span style=\"display:table;margin:0 auto;\">\n$$y = X\\theta$$\n</span>\n\n<span style=\"display:table;margin:0 auto;\">\n$${\n\\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{pmatrix}\n} =\n{\n\\begin{pmatrix}\nx_{11} & \\cdots & x_{1p} \\\\\nx_{21} & \\cdots & x_{2p} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nx_{n1} & \\cdots & x_{np}\n\\end{pmatrix}\n} {\n\\begin{pmatrix}\n\\theta_1 \\\\\n\\theta_2 \\\\\n\\vdots \\\\\n\\theta_p\n\\end{pmatrix}\n}\n$$\n</span>\n\n<span style=\"display:table;margin:0 auto;\">\n$$\n\\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{pmatrix}=\n\\begin{pmatrix}\nx_{11}\\theta_1+ x_{12}\\theta_2+\\cdots+x_{1p}\\theta_p \\\\\nx_{21}\\theta_1+x_{22}\\theta_2+\\cdots+x_{2p}\\theta_p \\\\\n\\vdots \\\\\nx_{n1}\\theta_1+x_{n2}+\\cdots+x_{np}\\theta_p\n\\end{pmatrix}\n$$</span>\n\n<p>This above equation is what we are trying to predict and the shape of the matrix is $$n \\times 1$$. I am sorry about being unable to name the equation, I am unable to figure out the way to do it. So this function passes through origin,but this isn&#39;t optimal. The process of adding a y-intercept will be left for the reader as an exercise. Here is a hint though.</p>\n<span style=\"display:table;margin:0 auto;\">\n$$y = X\\theta + \\varepsilon$$\n</span>\n\n<span style=\"display:table;margin:0 auto;\">\n$$\n\\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{pmatrix} = \n\\begin{pmatrix}\nx_{11} & \\cdots & x_{1p} \\\\\nx_{21} & \\cdots & x_{2p} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nx_{n1} & \\cdots & x_{np}\n\\end{pmatrix} \n\\begin{pmatrix}\n\\theta_1 \\\\\n\\theta_2 \\\\\n\\vdots \\\\\n\\theta_p\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n\\varepsilon_1 \\\\\n\\varepsilon_2 \\\\\n\\vdots \\\\\n\\varepsilon_n \\\\\n\\end{pmatrix}\n$$\n</span>\n\n<p>Enough with the maths jargon lets get to the code. Okay so lets make a structure that will represent the linear regression. Calling it <code>Linear</code> will be good right?</p>\n<pre><code class=\"hljs language-rust\"><span class=\"hljs-keyword\">pub</span> <span class=\"hljs-keyword\">struct</span> <span class=\"hljs-title class_\">Linear</span> {\n    data: Array2&lt;<span class=\"hljs-type\">f64</span>&gt;,\n    theta: Array2&lt;<span class=\"hljs-type\">f64</span>&gt;,\n    label: Array2&lt;<span class=\"hljs-type\">f64</span>&gt;\n}\n</code></pre>\n<p>We will be using <em><strong>Rust</strong></em> and a crate called <a href=\"https://crates.io/crates/ndarray\"><code>ndarray</code></a>. <code>ndarray</code> is great and intuative. I had worked with <code>numpy</code> before and <code>ndarray</code> is similar to it (but not really similar). Our data structure(<code>Linear</code>) contains a <code>data</code> array, a <code>theta</code> array and a <code>label</code> array. The <code>label</code> array will be used during training. The <code>theta</code> represents the parameter that our model will learn and this <code>theta</code> will be used to predict from the given data. Lets implement the <code>Linear</code> regression struct step by step.</p>\n<pre><code class=\"hljs language-rust\"><span class=\"hljs-keyword\">impl</span> <span class=\"hljs-title class_\">Linear</span> {\n    <span class=\"hljs-keyword\">pub</span> <span class=\"hljs-keyword\">fn</span> <span class=\"hljs-title function_\">new</span>(data: Array2&lt;<span class=\"hljs-type\">f64</span>&gt;, label: Array2&lt;<span class=\"hljs-type\">f64</span>&gt;)<span class=\"hljs-punctuation\">-&gt;</span> <span class=\"hljs-keyword\">Self</span> {\n        <span class=\"hljs-keyword\">let</span> <span class=\"hljs-variable\">theta</span> = Array2::<span class=\"hljs-title function_ invoke__\">from_shape_fn</span>((data.<span class=\"hljs-title function_ invoke__\">ncols</span>(),<span class=\"hljs-number\">1</span>),|(_,_)| rand::<span class=\"hljs-title function_ invoke__\">random</span>());\n        Linear {data, theta, label}\n    }\n}\n</code></pre>\n<p>Here we created a <code>new</code> function that creates a <code>Linear</code> struct. I couldn&#39;t create a  empty random matrix without using another crate, so here I am using a little hack(not really a hack though) to create a matrix of size $$n \\times 1$$. \nNow on to the most important and the simplest part. Every code after this point will be written inside the <code>impl</code> block.</p>\n<pre><code class=\"hljs language-rust\"><span class=\"hljs-keyword\">fn</span> <span class=\"hljs-title function_\">hypothesis</span>(&amp;<span class=\"hljs-keyword\">self</span>) <span class=\"hljs-punctuation\">-&gt;</span> Array2&lt;<span class=\"hljs-type\">f64</span>&gt; {\n    <span class=\"hljs-keyword\">let</span> <span class=\"hljs-variable\">prediction</span> = <span class=\"hljs-keyword\">self</span>.data.<span class=\"hljs-title function_ invoke__\">dot</span>(&amp;<span class=\"hljs-keyword\">self</span>.theta);\n    prediction\n}\n</code></pre>\n<p>Here we do our formula of matrix multiplication ($$y = X\\theta$$). We haven&#39;t talked about what inner product is, but we don&#39;t need a complete definition of it here. For our sake, inner product is the dot product for vector and matrix multiplication for matrix.</p>\n<pre><code class=\"hljs language-rust\"><span class=\"hljs-keyword\">fn</span> <span class=\"hljs-title function_\">gradient</span>(&amp;<span class=\"hljs-keyword\">mut</span> <span class=\"hljs-keyword\">self</span>, gamma: <span class=\"hljs-type\">f64</span>, iter: <span class=\"hljs-type\">i32</span>) {\n    <span class=\"hljs-keyword\">for</span> <span class=\"hljs-variable\">_i</span> <span class=\"hljs-keyword\">in</span> <span class=\"hljs-number\">1</span>..iter{\n        <span class=\"hljs-keyword\">let</span> <span class=\"hljs-variable\">delta</span> = ((<span class=\"hljs-keyword\">self</span>.<span class=\"hljs-title function_ invoke__\">hypo</span>() - &amp;<span class=\"hljs-keyword\">self</span>.label).<span class=\"hljs-title function_ invoke__\">reversed_axes</span>().<span class=\"hljs-title function_ invoke__\">dot</span>(&amp;<span class=\"hljs-keyword\">self</span>.data)).<span class=\"hljs-title function_ invoke__\">reversed_axes</span>() * (<span class=\"hljs-number\">1</span>/<span class=\"hljs-keyword\">self</span>.data.<span class=\"hljs-title function_ invoke__\">nrows</span>()) * gamma;\n        <span class=\"hljs-keyword\">self</span>.theta = &amp;<span class=\"hljs-keyword\">self</span>.theta - delta;\n    }\n}\n</code></pre>\n<p>The initialization of <code>delta</code> variable looks complex, but I promise if I had splited it into multiple lines, it would have looked even more worse. On that note, lets go step by step on how it it initialized.\nThe <code>delta</code> here is the vectorized form of gradient descent that we saw earlier. Okay so now a bit of maths :). </p>\n<p>The cost function for linear regression is defined as:</p>\n<span style=\"display:table;margin:0 auto;\">\n$$\nC = \\frac{1}{2m}\\displaystyle\\sum_{i=1}^m(h_\\theta\\relax(x_i)-y_i)^2\n$$\n</span>\n\n<p>This is the cost function which is the average of loss function(this is why we have $$\\frac{1}{2m}$$ there. The $$2$$ doesn&#39;t matter, it is there to make the equation look prettier after we take the derivative). Here $$m$$ is the number of rows. So, the gradient descent will optimize according to this cost function. Taking the derivative wrt. each $$\\theta$$. You can check out the derivation on the bottom of the page. I cannot guarantee that it is correct, as it is something I did, but it is intuitive?</p>\n<span style=\"display:table;margin:0 auto;\">\n$$\n\\frac{\\delta C}{\\delta \\theta} = \\frac{1}{m}\\displaystyle\\sum_{i=1}^m(h_\\theta\\relax(x_i)-y_i)x\n$$\n</span>\n\n<p>And then finally, relating this to the code above. <code>(self.hypo() - &amp;self.label)</code> is the $$(h_\\theta\\relax(x_i)-y_i)$$ part. Then we <code>reversed_axis</code>, which is done to allow the multiplication between the afforementioned value and $$x$$. So this code <code>(self.hypo()-&amp;self.label).reversed_axes().dot(&amp;self.data)).reversed_axes()</code> is equivalent to the following expression.</p>\n<span style=\"display:table;margin:0 auto;\">\n$$\n\\displaystyle\\sum_{i=1}^m(h_\\theta\\relax(x_i)-y_i)x\n$$\n</span>\n\n<p>Then, we divide it by <code>self.data.nrows()</code> as in the equation to get the final value. The extra <code>gamma</code> parameter is the step-size as in the equation</p>\n<p><span style=\"display:table;margin:0 auto;\">$$x_{n+1} = x_n - \\gamma_n \\varDelta F(\\relax{x})$$</span></p>\n<p>Now the final step, taking it all together. We subtract the <code>delta</code> from <code>self.theta</code> to get the new theta value. This is equivalent to the equation above. In terms of our code, the mathematical equation is given as:</p>\n<p><span style=\"display:table;margin:0 auto;\">$$\\theta_{n+1} = \\theta_n - \\gamma \\frac{\\delta C}{\\delta \\theta}$$</span></p>\n<p>The <code>iter</code> parameter is the number of iteration, surprised? Okay now the final part, the training. Lets create a training function.</p>\n<pre><code class=\"hljs language-rust\"><span class=\"hljs-keyword\">pub</span> <span class=\"hljs-keyword\">fn</span> <span class=\"hljs-title function_\">train</span>(&amp;<span class=\"hljs-keyword\">mut</span> <span class=\"hljs-keyword\">self</span>, alpha: <span class=\"hljs-type\">f64</span>, iter: <span class=\"hljs-type\">i32</span>) {\n        <span class=\"hljs-keyword\">self</span>.<span class=\"hljs-title function_ invoke__\">gradient</span>(alpha, iter);\n}\n</code></pre>\n<p>This calls the <code>gradient</code> function with those parameters and it&#39;s done!! To predict results after training we can create a public function named <code>predict</code> and inner product the input data with the <code>theta</code> parameter in our struct.</p>\n<pre><code class=\"hljs language-rust\"><span class=\"hljs-keyword\">pub</span> <span class=\"hljs-keyword\">fn</span> <span class=\"hljs-title function_\">predict</span>(&amp;<span class=\"hljs-keyword\">self</span>, input: Array2&lt;<span class=\"hljs-type\">f64</span>&gt;) <span class=\"hljs-punctuation\">-&gt;</span> Array2&lt;<span class=\"hljs-type\">f64</span>&gt; {\n        input.<span class=\"hljs-title function_ invoke__\">dot</span>(&amp;<span class=\"hljs-keyword\">self</span>.theta)\n}\n</code></pre>\n<p>And this is it. Everything is done. This is the simplest linear regression, now we can train and test it. The code for training and testing it is given below:</p>\n<pre><code class=\"hljs language-rust\"><span class=\"hljs-keyword\">use</span> ndarray::{<span class=\"hljs-keyword\">self</span>, Array, Array2, Ix2};\n\n<span class=\"hljs-keyword\">fn</span> <span class=\"hljs-title function_\">main</span>() {\n    <span class=\"hljs-keyword\">let</span> <span class=\"hljs-variable\">data</span> = Array::<span class=\"hljs-title function_ invoke__\">range</span>(<span class=\"hljs-number\">1.0</span>, <span class=\"hljs-number\">15.0</span>, <span class=\"hljs-number\">1.0</span>).<span class=\"hljs-title function_ invoke__\">into_shape</span>((<span class=\"hljs-number\">14</span>, <span class=\"hljs-number\">1</span>)).<span class=\"hljs-title function_ invoke__\">unwrap</span>();\n    <span class=\"hljs-keyword\">let</span> <span class=\"hljs-variable\">label</span> = Array::<span class=\"hljs-title function_ invoke__\">range</span>(<span class=\"hljs-number\">1.0</span>, <span class=\"hljs-number\">15.0</span>, <span class=\"hljs-number\">1.0</span>).<span class=\"hljs-title function_ invoke__\">into_shape</span>((<span class=\"hljs-number\">14</span>, <span class=\"hljs-number\">1</span>)).<span class=\"hljs-title function_ invoke__\">unwrap</span>();\n    <span class=\"hljs-keyword\">let</span> <span class=\"hljs-keyword\">mut </span><span class=\"hljs-variable\">lin</span> = Linear::<span class=\"hljs-title function_ invoke__\">new</span>(data, label);\n    lin.<span class=\"hljs-title function_ invoke__\">train</span>(<span class=\"hljs-number\">0.01</span>, <span class=\"hljs-number\">3</span>);\n    <span class=\"hljs-keyword\">let</span> <span class=\"hljs-variable\">data</span> = Array::<span class=\"hljs-title function_ invoke__\">range</span>(<span class=\"hljs-number\">11.0</span>, <span class=\"hljs-number\">25.0</span>, <span class=\"hljs-number\">1.0</span>).<span class=\"hljs-title function_ invoke__\">into_shape</span>((<span class=\"hljs-number\">14</span>, <span class=\"hljs-number\">1</span>)).<span class=\"hljs-title function_ invoke__\">unwrap</span>();\n    <span class=\"hljs-built_in\">println!</span>(<span class=\"hljs-string\">&quot;{:?}&quot;</span>, lin.<span class=\"hljs-title function_ invoke__\">predict</span>(data));\n}\n</code></pre>\n<p>There are lot of roads one can go from here. We could add a error-var or create a different optimization algorithm. Thank you for reading, have a great day(or night). \nThe derivation of the loss function is given as. Here, when we derivate the $$\\theta$$ is $$\\theta_j$$ and the equation is $$h_\\theta\\relax(x_i) = \\theta_j X$$  but for simplicity sake we will be putting it as $$\\theta$$ and $$h_\\theta\\relax(x_i) = \\theta X$$.</p>\n<span style=\"display:table;margin:0 auto;\">\n$$\nC = \\frac{1}{2m}\\displaystyle\\sum_{i=1}^m(h_\\theta\\relax(x_i)-y_i)^2\n$$\n</span>\n\n<p>$$\nTaking&gt; derivation&gt; wrt &gt; &quot;\\theta&quot;,\n$$</p>\n<span style=\"display:table;margin:0 auto;\">\n$$\n\\frac{\\delta C}{\\delta \\theta} = \\frac{\\frac{1}{2m}\\displaystyle\\sum_{i=1}^m(h_\\theta\\relax(x_i)-y_i)^2}{d\\theta}\n$$\n</span>\n\n<span style=\"display:table;margin:0 auto;\">\n$$\n\\frac{\\delta C}{\\delta \\theta} = \\frac{1}{2m}\\displaystyle\\sum_{i=1}^m\\frac{(h_\\theta\\relax(x_i)-y_i)^2}{d\\theta}\n$$\n</span>\n\n<span style=\"display:table;margin:0 auto;\">\n$$\n\\frac{\\delta C}{\\delta \\theta} = \\frac{1}{2m}\\displaystyle\\sum_{i=1}^m\\frac{(h_\\theta\\relax(x_i)-y_i)^2}{d(h_\\theta\\relax(x_i)-y_i)}\\times\\frac{d(h_\\theta\\relax(x_i)-y_i)}{d\\theta}\n$$\n</span>\n\n<span style=\"display:table;margin:0 auto;\">\n$$\n\\frac{\\delta C}{\\delta \\theta} = \\frac{1}{\\cancel{2}m}\\times\\cancel{2}\\times\\displaystyle\\sum_{i=1}^m(h_\\theta\\relax(x_i)-y_i) \\lparen \\frac{dh_\\theta\\relax(x_i)}{d\\theta} - \\frac{dy_i}{d\\theta}\\rparen\n$$\n</span>\n\n<p>$$We &gt; know &gt; h_\\theta\\relax(x_i) = \\theta X &gt; and &gt; \\frac{dy_i}{d\\theta} = 0,$$</p>\n<span style=\"display:table;margin:0 auto;\">\n$$\n\\frac{\\delta C}{\\delta \\theta} = \\frac{1}{m}\\displaystyle\\sum_{i=1}^m(h_\\theta\\relax(x_i)-y_i)(\\frac{\\cancel{d\\theta} X}{\\cancel{d\\theta}})\n$$\n</span>\n\n<p>$$So &gt; we &gt; have,$$</p>\n<span style=\"display:table;margin:0 auto;\">\n$$\n\\frac{\\delta C}{\\delta \\theta} = \\frac{1}{m}\\displaystyle\\sum_{i=1}^m(h_\\theta\\relax(x_i)-y_i)X\n$$\n</span>\n","bodyBegin":5,"frontmatter":"title: Simple Linear Regression For Simple Soul\ndate: 2021-07-05"},"prev":{"link":"2021-06-21-random_or_not_to_be_random","blog":{"date":"2021-06-21","title":"Random Or Not to be Random"}},"next":{"link":"2021-07-15-polynomial","blog":{"date":"2021-07-15","title":"Polynomial"}}}}