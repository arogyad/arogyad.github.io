<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<link rel="icon" href="/favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<title>Autograd in Rust</title><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/styles/base16/gruvbox-light-medium.min.css" integrity="sha512-YOLBTZnIcnB3qm7sPFlGHx0no3yEGWvVTAI9uA6uaZGUBQui/DP9vh0FLmCJbOL5TxHJWwaiI23cCEJsmIeMew==" crossorigin="anonymous" referrerpolicy="no-referrer" data-svelte="svelte-8dvywl"><script src="https://use.fontawesome.com/c5d5ee5034.js" data-svelte="svelte-zlrjvv"></script>

		

		<link rel="modulepreload" href="/_app/start-d1d3ff06.js">
		<link rel="modulepreload" href="/_app/chunks/vendor-5b6c161e.js">
		<link rel="modulepreload" href="/_app/pages/__layout.svelte-77b01ce5.js">
		<link rel="modulepreload" href="/_app/pages/index.svelte-f8b2b4ac.js">
		<link rel="modulepreload" href="/_app/chunks/BlogShow-4299f9c2.js">
		<link rel="stylesheet" href="/_app/assets/start-464e9d0a.css">
		<link rel="stylesheet" href="/_app/assets/pages/__layout.svelte-b7ed98bd.css">

		<script type="module">
			import { start } from "/_app/start-d1d3ff06.js";
			start({
				target: document.querySelector("#svelte"),
				paths: {"base":"","assets":""},
				session: {},
				host: location.host,
				route: true,
				spa: false,
				trailing_slash: "never",
				hydrate: {
					status: 200,
					error: null,
					nodes: [
						import("/_app/pages/__layout.svelte-77b01ce5.js"),
						import("/_app/pages/index.svelte-f8b2b4ac.js")
					],
					page: {
						host: location.host, // TODO this is redundant
						path: "/",
						query: new URLSearchParams(""),
						params: {}
					}
				}
			});
		</script>
	</head>
	<body>
		<div id="svelte">






<div id="sidebar" class="md:h-full md:w-60 md:fixed bg-green-900 dark:bg-black text-white duration-300 text-center text-sm"><div id="items" class="py-4 md:pt-40 grid text-center"><a href="/" class="font-bold text-6xl font-serif hover:underline text-white">Arogya Dahal</a>
		<span class="font-extralight p-5 text-white">Welcome to my blog!</span>
		<a href="/" class="font-normal p-1 text-xl text-white hover:text-2xl duration-300">Home</a>
		<a href="/about" class="font-normal md:pb-1 text-xl text-white hover:text-2xl duration-300">About</a>
		<a href="/blogs" class="font-normal text-xl text-white hover:text-2xl duration-300">Blogs</a></div>
	<a href="https://github.com/arogyad/" target="_blank"><i class="fa fa-github fa-2x hover:text-4xl text-white duration-200 md:ml-20"></i></a>
	<a href="https://instagram.com/arogyad/" target="_blank"><i class="fa fa-instagram fa-2x hover:text-4xl text-white duration-200 m-4"></i></a></div>
<button class="float-right">a </button>




<div id="blog" class="md:pl-72 md:pr-10 md:pt-10 p-10 md:text-justify"><h1 class="md:">Autograd in Rust</h1>
	<span class="font-light">2021-08-12</span>
	<!-- HTML_TAG_START --><h1 id="introduction">Introduction</h1>
<p>In my quest for learning <em><strong>Rust</strong></em>, I had started writing a machine learning library. Machine learning is pretty interesting , but deep learning takes the cake when it comes to the weirdness. One main thing that is required for a deep learning library is a auto differentiation engine. An autodiff engine is a piece of code that sits on top and creates a graph for how the backward propagation should be done. I wanted a autograd library for my project so tried writing it. I was more interested in dynamic graphsish autograd style something like that of <a href="https://pytorch.org/" target='_blank'> pytorch </a> rather than the static graph of tensorflow. Pytorch specifically creates a <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph" target='_blank'>DAG graph</a>. The reverse topilogically sorted Directed Graph (DAG) is ,then, used to create a graph for backward propagation. <a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html" target="_blank">Here</a> is a link for further reading on the autograd of pytorch. This blog is quite long as there is a lot to talk about.</p>
<h1 id="background">Background</h1>
<p>The implementation is heavly inspired from <a href="https://github.com/geohot/tinygrad/" target="_blank"> tinygrad </a>  and pytorch. The implementation shown here is an extremely small part, and a great deal of other fields and methods are yet to be added. Here, we will be creating a autograd library in rust. We will be using <code>f64</code> values, but we can easily replace that with anything else. The use of <code>f64</code> here is just for easier explanation. One difference when compared to the tinygrad&#39;s implementation is that, due to the nature of no garbage collection language, we don&#39;t need to store the <code>parents</code> and the <code>saved_tensor</code> in two objects, but we will get to that point in the future.</p>
<h1 id="tensor-class">&quot;Tensor&quot; Class</h1>
<p>It gives me immense guilt to call the &quot;Tensor&quot; struct here as the &quot;Tensor&quot; as the bar for something to be called a tensor is too high. It is nothing but the absolute minimum that we require to create and show a autograd engine. The &quot;Tensor&quot; struct is defined as such:</p>
<pre><code class="hljs language-rust"><span class="hljs-comment">// /src/tensor.rs/Tensor</span>
<span class="hljs-keyword">pub</span> <span class="hljs-keyword">struct</span> <span class="hljs-title class_">Tensor</span>&lt;<span class="hljs-symbol">&#x27;a</span>&gt; {
    <span class="hljs-keyword">pub</span> data: <span class="hljs-type">f64</span>,
    <span class="hljs-keyword">pub</span> grad: Cell&lt;<span class="hljs-type">f64</span>&gt;,
    <span class="hljs-keyword">pub</span> _ctx: <span class="hljs-type">Option</span>&lt;<span class="hljs-type">Box</span>&lt;&amp;<span class="hljs-symbol">&#x27;a</span> <span class="hljs-keyword">dyn</span> Function&gt;&gt;,
}
</code></pre>
<p>This isn&#39;t much but this does the work done, atleast for the explanation.  The <code>_ctx</code> is the context field, which stores the process of creation of the <code>self</code> tensor, other are self explanatory. You might be wondering why the <code>_ctx</code> is a <code>Box&lt;&amp;&#39; dyn Function&gt;</code> and not a generic-it is easier to read and simpler this way, using generics will increase the complexity without much gain (also I don&#39;t know if will generics work). The <code>Tensor</code> declares a <code>new</code> static function which is self explanatory. You can check out the code on my <a href="https://github.com/arogyad/autograd" target="_blank">github</a>. We will talk about other functions as we go along the way.</p>
<h2 id="where-does-the-_ctx-come-from-">Where does the &quot;_ctx&quot; come from ?!</h2>
<p>The <code>_ctx</code> is an object which implements the <code>Function</code> trait. The function trait is defined as such</p>
<pre><code class="hljs language-rust"><span class="hljs-comment">// /src/ftrait.rs/Function</span>
<span class="hljs-keyword">pub</span> <span class="hljs-keyword">trait</span> <span class="hljs-title class_">Function</span> {
    <span class="hljs-keyword">fn</span> <span class="hljs-title function_">apply</span>(&amp;<span class="hljs-keyword">self</span>) <span class="hljs-punctuation">-&gt;</span> Tensor;
    <span class="hljs-keyword">fn</span> <span class="hljs-title function_">backward</span>(&amp;<span class="hljs-keyword">self</span>, grad: <span class="hljs-type">f64</span>) <span class="hljs-punctuation">-&gt;</span> [<span class="hljs-type">f64</span>; <span class="hljs-number">2</span>];
    <span class="hljs-keyword">fn</span> <span class="hljs-title function_">forward</span>(&amp;<span class="hljs-keyword">self</span>) <span class="hljs-punctuation">-&gt;</span> <span class="hljs-type">f64</span>;
    <span class="hljs-keyword">fn</span> <span class="hljs-title function_">parents</span>(&amp;<span class="hljs-keyword">self</span>) <span class="hljs-punctuation">-&gt;</span> [&amp;Tensor; <span class="hljs-number">2</span>];
}
</code></pre>
<p>The <code>parents</code> function returns the parent inside of the struct, which implements the <code>Function</code> trait. This is important for the creation of graph during backward propagation. The function trait here is defined for binary operations, but we could have made it for other operations as well by using maybe a const generic type( <code>&lt;const X: i32&gt;</code> )?! <code>Apply</code> performs the <code>forward</code> function and returns the required tensor containing its own context(We will see this later). The <code>backward</code> function performs the backward computation,i.e it calculates the grad based on the incoming <code>grad</code>(chain rule). We will implement the <code>Function</code> trait for multiplication type. We won&#39;t be overriding any operator functions such as <code>*</code> operator. This will show us the bare metal, how things are working. Lets look at the implementation of <code>Mul</code>. <code>Forward</code> and <code>Backward</code> are trivial so we will be looking at the <code>apply</code> function&#39;s implementation. <code>Mul</code> struct is defined as</p>
<pre><code class="hljs language-rust"><span class="hljs-comment">// /src/Function.rs/Mul</span>
<span class="hljs-keyword">pub</span> <span class="hljs-keyword">struct</span> <span class="hljs-title class_">Mul</span>&lt;<span class="hljs-symbol">&#x27;a</span>&gt; {
    parents: [&amp;<span class="hljs-symbol">&#x27;a</span> Tensor&lt;<span class="hljs-symbol">&#x27;a</span>&gt;; <span class="hljs-number">2</span>],
}

<span class="hljs-comment">// Inside the impl block of Mul for Function</span>
<span class="hljs-keyword">fn</span> <span class="hljs-title function_">apply</span>(&amp;<span class="hljs-keyword">self</span>) <span class="hljs-punctuation">-&gt;</span> Tensor {
    <span class="hljs-keyword">let</span> <span class="hljs-variable">_ret</span> = Tensor::<span class="hljs-title function_ invoke__">new</span>(<span class="hljs-keyword">self</span>.<span class="hljs-title function_ invoke__">forward</span>(), <span class="hljs-title function_ invoke__">Some</span>(Box::<span class="hljs-title function_ invoke__">new</span>(<span class="hljs-keyword">self</span>)));
    _ret
} 
</code></pre>
<p>So, <code>apply</code> return a <code>Tensor</code> with the value from the <code>self.forward()</code> and <code>_ctx</code> equal to <code>Some(Box::new(self))</code>. The context stores the its <code>parents</code>, this is how we know where a <code>Tensor</code> come from, and this is how we can create a backward graph.</p>
<h1 id="backward">Backward</h1>
<p>The backward pass consists of the creation of a DAG graph using the final tensor, and going up the graph and supplying everyone with their <code>grad</code> value. A filter would be good to limit whose grad to calculate,pass in the <code>backward</code> function,so this is left to the reader as an exercise(Calculate the grad if require_grad is <code>true</code>?). We <a href="https://en.wikipedia.org/wiki/Topological_sorting" target="_blank">topo</a> sort it. </p>
<pre><code class="hljs language-rust"><span class="hljs-keyword">fn</span> <span class="hljs-title function_">_deepwalk</span>(node: &amp;<span class="hljs-symbol">&#x27;a</span> Tensor&lt;<span class="hljs-symbol">&#x27;a</span>&gt;, nodes: &amp;<span class="hljs-symbol">&#x27;_</span> <span class="hljs-keyword">mut</span> <span class="hljs-type">Vec</span>&lt;&amp;<span class="hljs-symbol">&#x27;a</span> Tensor&lt;<span class="hljs-symbol">&#x27;a</span>&gt;&gt;) {
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">let</span> <span class="hljs-variable">Some</span>(n) = &amp;node._ctx {
        <span class="hljs-keyword">for</span> <span class="hljs-variable">i</span> <span class="hljs-keyword">in</span> n.<span class="hljs-title function_ invoke__">parents</span>() {
            <span class="hljs-keyword">Self</span>::_deepwalk(i, nodes);
        }
        nodes.<span class="hljs-title function_ invoke__">push</span>(node);
    }
}
</code></pre>
<p>This is a recursive toposorter?(Completely inspired from<a href="https://github.com/geohot/tinygrad/" target="_blank"> here</a>) which looks readable, unlike the one written in python(no hard feelings! I am extremely bad at reading python), and is pretty good without a filter for visited(will have to add the eventually). And, we calculate the backward pass gradients and provide it to the necessary <code>Tensor</code>. I don&#39;t want this to be page to be congested with codes, so please check it out on the github repo. The loop in the <code>backward</code> function of the <code>Tensor</code> loops in reverse topo order every tensor inside the vec returned by <code>self.walk()</code>(Refer to github). We then apply the chain rule with every tensor getting it&#39;s grad from the tensor after(or is it before?) it. This is how the autograd process works, pretty simple; however, my explanation might have made it harder. We can expand this to use a <code>array</code> or <code>vec</code> or arrayfire&#39;s array, which is the library I am using for my project <a href="https://github.com/arogyad/Candle" target="_blank">Candle</a>. And finally lets look at how all this things occur. Lets write a test for a linear equation with the comments acting as the explanation. Thank you for reading!!</p>
<pre><code class="hljs language-rust"><span class="hljs-meta">#[test]</span>
<span class="hljs-keyword">fn</span> <span class="hljs-title function_">linear_test</span>() {
    <span class="hljs-comment">// We create two tensors</span>
    <span class="hljs-keyword">let</span> <span class="hljs-variable">a</span> = Tensor::<span class="hljs-title function_ invoke__">new</span>(<span class="hljs-number">2.0</span>, <span class="hljs-literal">None</span>); <span class="hljs-comment">// &#x27;_ctx&#x27; as None as this is formed by you and me not an operation </span>
    <span class="hljs-keyword">let</span> <span class="hljs-variable">b</span> = Tensor::<span class="hljs-title function_ invoke__">new</span>(<span class="hljs-number">3.0</span>, <span class="hljs-literal">None</span>); <span class="hljs-comment">// Same</span>

    <span class="hljs-comment">// Lets create a multiplication context which will contain tensor &#x27;a&#x27; and &#x27;b&#x27; as its parents</span>
    <span class="hljs-keyword">let</span> <span class="hljs-variable">c_ctx</span> = Mul::<span class="hljs-title function_ invoke__">new</span>([&amp;a, &amp;b]);

    <span class="hljs-comment">// Lets create the Tensor with the context &#x27;c_ctx&#x27;</span>
    <span class="hljs-keyword">let</span> <span class="hljs-variable">c</span> = c_ctx.<span class="hljs-title function_ invoke__">apply</span>();

    <span class="hljs-comment">// Let&#x27;s create the bais </span>
    <span class="hljs-keyword">let</span> <span class="hljs-variable">d</span> = Tensor::<span class="hljs-title function_ invoke__">new</span>(<span class="hljs-number">4.0</span>, <span class="hljs-literal">None</span>); <span class="hljs-comment">// Same as before the `_ctx` as None</span>

    <span class="hljs-comment">// Lets create a addition context and apply it to obtain the final output y</span>
    <span class="hljs-keyword">let</span> <span class="hljs-variable">e_ctx</span> = Add::<span class="hljs-title function_ invoke__">new</span>([&amp;c, &amp;d]);
    <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">e</span>  = e_ctx.<span class="hljs-title function_ invoke__">apply</span>();

    <span class="hljs-comment">// Backpropagation</span>
    e.<span class="hljs-title function_ invoke__">backward</span>();

    <span class="hljs-comment">// Will this assertion work?!</span>
    <span class="hljs-built_in">assert!</span>(a.grad.<span class="hljs-title function_ invoke__">get</span>() == <span class="hljs-number">3.0</span>); <span class="hljs-comment">// The grads are inside Cell btw</span>
    <span class="hljs-built_in">assert!</span>(b.grad.<span class="hljs-title function_ invoke__">get</span>() == <span class="hljs-number">2.0</span>); <span class="hljs-comment">// It was done to make the grad changing easier</span>
} 
</code></pre>
<!-- HTML_TAG_END -->
	<div id="pad" class="pt-8"><div id="next" class="flex pt-2 border-t-2"><div id="pr" class="w-2/4 flex-1 hover:text-green-800"><i class="fa fa-caret-left fa-sm"></i>
			<a href="/2021-07-15-polynomial" class="text-xl font-light">Polynomial <span class="font-light text-sm">[2021-07-15]</span></a></div>
	<div id="nxt" class="w-2/4 hover:text-red-800 text-right"><span class="float-right text-xl font-light text-red-800">Newest</span></div></div></div></div>



			<script type="application/json" data-type="svelte-data" data-url="/from_index.json">{"status":200,"statusText":"","headers":{"content-type":"application/json; charset=utf-8"},"body":"{\"content\":{\"out\":{\"attributes\":{\"title\":\"Autograd in Rust\",\"date\":\"2021-08-12\"},\"body\":\"\u003Ch1 id=\\\"introduction\\\"\u003EIntroduction\u003C\u002Fh1\u003E\\n\u003Cp\u003EIn my quest for learning \u003Cem\u003E\u003Cstrong\u003ERust\u003C\u002Fstrong\u003E\u003C\u002Fem\u003E, I had started writing a machine learning library. Machine learning is pretty interesting , but deep learning takes the cake when it comes to the weirdness. One main thing that is required for a deep learning library is a auto differentiation engine. An autodiff engine is a piece of code that sits on top and creates a graph for how the backward propagation should be done. I wanted a autograd library for my project so tried writing it. I was more interested in dynamic graphsish autograd style something like that of \u003Ca href=\\\"https:\u002F\u002Fpytorch.org\u002F\\\" target='_blank'\u003E pytorch \u003C\u002Fa\u003E rather than the static graph of tensorflow. Pytorch specifically creates a \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FDirected_acyclic_graph\\\" target='_blank'\u003EDAG graph\u003C\u002Fa\u003E. The reverse topilogically sorted Directed Graph (DAG) is ,then, used to create a graph for backward propagation. \u003Ca href=\\\"https:\u002F\u002Fpytorch.org\u002Ftutorials\u002Fbeginner\u002Fblitz\u002Fautograd_tutorial.html\\\" target=\\\"_blank\\\"\u003EHere\u003C\u002Fa\u003E is a link for further reading on the autograd of pytorch. This blog is quite long as there is a lot to talk about.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"background\\\"\u003EBackground\u003C\u002Fh1\u003E\\n\u003Cp\u003EThe implementation is heavly inspired from \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fgeohot\u002Ftinygrad\u002F\\\" target=\\\"_blank\\\"\u003E tinygrad \u003C\u002Fa\u003E  and pytorch. The implementation shown here is an extremely small part, and a great deal of other fields and methods are yet to be added. Here, we will be creating a autograd library in rust. We will be using \u003Ccode\u003Ef64\u003C\u002Fcode\u003E values, but we can easily replace that with anything else. The use of \u003Ccode\u003Ef64\u003C\u002Fcode\u003E here is just for easier explanation. One difference when compared to the tinygrad&#39;s implementation is that, due to the nature of no garbage collection language, we don&#39;t need to store the \u003Ccode\u003Eparents\u003C\u002Fcode\u003E and the \u003Ccode\u003Esaved_tensor\u003C\u002Fcode\u003E in two objects, but we will get to that point in the future.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"tensor-class\\\"\u003E&quot;Tensor&quot; Class\u003C\u002Fh1\u003E\\n\u003Cp\u003EIt gives me immense guilt to call the &quot;Tensor&quot; struct here as the &quot;Tensor&quot; as the bar for something to be called a tensor is too high. It is nothing but the absolute minimum that we require to create and show a autograd engine. The &quot;Tensor&quot; struct is defined as such:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"hljs language-rust\\\"\u003E\u003Cspan class=\\\"hljs-comment\\\"\u003E\u002F\u002F \u002Fsrc\u002Ftensor.rs\u002FTensor\u003C\u002Fspan\u003E\\n\u003Cspan class=\\\"hljs-keyword\\\"\u003Epub\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-keyword\\\"\u003Estruct\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-title class_\\\"\u003ETensor\u003C\u002Fspan\u003E&lt;\u003Cspan class=\\\"hljs-symbol\\\"\u003E&#x27;a\u003C\u002Fspan\u003E&gt; {\\n    \u003Cspan class=\\\"hljs-keyword\\\"\u003Epub\u003C\u002Fspan\u003E data: \u003Cspan class=\\\"hljs-type\\\"\u003Ef64\u003C\u002Fspan\u003E,\\n    \u003Cspan class=\\\"hljs-keyword\\\"\u003Epub\u003C\u002Fspan\u003E grad: Cell&lt;\u003Cspan class=\\\"hljs-type\\\"\u003Ef64\u003C\u002Fspan\u003E&gt;,\\n    \u003Cspan class=\\\"hljs-keyword\\\"\u003Epub\u003C\u002Fspan\u003E _ctx: \u003Cspan class=\\\"hljs-type\\\"\u003EOption\u003C\u002Fspan\u003E&lt;\u003Cspan class=\\\"hljs-type\\\"\u003EBox\u003C\u002Fspan\u003E&lt;&amp;\u003Cspan class=\\\"hljs-symbol\\\"\u003E&#x27;a\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-keyword\\\"\u003Edyn\u003C\u002Fspan\u003E Function&gt;&gt;,\\n}\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThis isn&#39;t much but this does the work done, atleast for the explanation.  The \u003Ccode\u003E_ctx\u003C\u002Fcode\u003E is the context field, which stores the process of creation of the \u003Ccode\u003Eself\u003C\u002Fcode\u003E tensor, other are self explanatory. You might be wondering why the \u003Ccode\u003E_ctx\u003C\u002Fcode\u003E is a \u003Ccode\u003EBox&lt;&amp;&#39; dyn Function&gt;\u003C\u002Fcode\u003E and not a generic-it is easier to read and simpler this way, using generics will increase the complexity without much gain (also I don&#39;t know if will generics work). The \u003Ccode\u003ETensor\u003C\u002Fcode\u003E declares a \u003Ccode\u003Enew\u003C\u002Fcode\u003E static function which is self explanatory. You can check out the code on my \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Farogyad\u002Fautograd\\\" target=\\\"_blank\\\"\u003Egithub\u003C\u002Fa\u003E. We will talk about other functions as we go along the way.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"where-does-the-_ctx-come-from-\\\"\u003EWhere does the &quot;_ctx&quot; come from ?!\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe \u003Ccode\u003E_ctx\u003C\u002Fcode\u003E is an object which implements the \u003Ccode\u003EFunction\u003C\u002Fcode\u003E trait. The function trait is defined as such\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"hljs language-rust\\\"\u003E\u003Cspan class=\\\"hljs-comment\\\"\u003E\u002F\u002F \u002Fsrc\u002Fftrait.rs\u002FFunction\u003C\u002Fspan\u003E\\n\u003Cspan class=\\\"hljs-keyword\\\"\u003Epub\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-keyword\\\"\u003Etrait\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-title class_\\\"\u003EFunction\u003C\u002Fspan\u003E {\\n    \u003Cspan class=\\\"hljs-keyword\\\"\u003Efn\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-title function_\\\"\u003Eapply\u003C\u002Fspan\u003E(&amp;\u003Cspan class=\\\"hljs-keyword\\\"\u003Eself\u003C\u002Fspan\u003E) \u003Cspan class=\\\"hljs-punctuation\\\"\u003E-&gt;\u003C\u002Fspan\u003E Tensor;\\n    \u003Cspan class=\\\"hljs-keyword\\\"\u003Efn\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-title function_\\\"\u003Ebackward\u003C\u002Fspan\u003E(&amp;\u003Cspan class=\\\"hljs-keyword\\\"\u003Eself\u003C\u002Fspan\u003E, grad: \u003Cspan class=\\\"hljs-type\\\"\u003Ef64\u003C\u002Fspan\u003E) \u003Cspan class=\\\"hljs-punctuation\\\"\u003E-&gt;\u003C\u002Fspan\u003E [\u003Cspan class=\\\"hljs-type\\\"\u003Ef64\u003C\u002Fspan\u003E; \u003Cspan class=\\\"hljs-number\\\"\u003E2\u003C\u002Fspan\u003E];\\n    \u003Cspan class=\\\"hljs-keyword\\\"\u003Efn\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-title function_\\\"\u003Eforward\u003C\u002Fspan\u003E(&amp;\u003Cspan class=\\\"hljs-keyword\\\"\u003Eself\u003C\u002Fspan\u003E) \u003Cspan class=\\\"hljs-punctuation\\\"\u003E-&gt;\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-type\\\"\u003Ef64\u003C\u002Fspan\u003E;\\n    \u003Cspan class=\\\"hljs-keyword\\\"\u003Efn\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-title function_\\\"\u003Eparents\u003C\u002Fspan\u003E(&amp;\u003Cspan class=\\\"hljs-keyword\\\"\u003Eself\u003C\u002Fspan\u003E) \u003Cspan class=\\\"hljs-punctuation\\\"\u003E-&gt;\u003C\u002Fspan\u003E [&amp;Tensor; \u003Cspan class=\\\"hljs-number\\\"\u003E2\u003C\u002Fspan\u003E];\\n}\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe \u003Ccode\u003Eparents\u003C\u002Fcode\u003E function returns the parent inside of the struct, which implements the \u003Ccode\u003EFunction\u003C\u002Fcode\u003E trait. This is important for the creation of graph during backward propagation. The function trait here is defined for binary operations, but we could have made it for other operations as well by using maybe a const generic type( \u003Ccode\u003E&lt;const X: i32&gt;\u003C\u002Fcode\u003E )?! \u003Ccode\u003EApply\u003C\u002Fcode\u003E performs the \u003Ccode\u003Eforward\u003C\u002Fcode\u003E function and returns the required tensor containing its own context(We will see this later). The \u003Ccode\u003Ebackward\u003C\u002Fcode\u003E function performs the backward computation,i.e it calculates the grad based on the incoming \u003Ccode\u003Egrad\u003C\u002Fcode\u003E(chain rule). We will implement the \u003Ccode\u003EFunction\u003C\u002Fcode\u003E trait for multiplication type. We won&#39;t be overriding any operator functions such as \u003Ccode\u003E*\u003C\u002Fcode\u003E operator. This will show us the bare metal, how things are working. Lets look at the implementation of \u003Ccode\u003EMul\u003C\u002Fcode\u003E. \u003Ccode\u003EForward\u003C\u002Fcode\u003E and \u003Ccode\u003EBackward\u003C\u002Fcode\u003E are trivial so we will be looking at the \u003Ccode\u003Eapply\u003C\u002Fcode\u003E function&#39;s implementation. \u003Ccode\u003EMul\u003C\u002Fcode\u003E struct is defined as\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"hljs language-rust\\\"\u003E\u003Cspan class=\\\"hljs-comment\\\"\u003E\u002F\u002F \u002Fsrc\u002FFunction.rs\u002FMul\u003C\u002Fspan\u003E\\n\u003Cspan class=\\\"hljs-keyword\\\"\u003Epub\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-keyword\\\"\u003Estruct\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-title class_\\\"\u003EMul\u003C\u002Fspan\u003E&lt;\u003Cspan class=\\\"hljs-symbol\\\"\u003E&#x27;a\u003C\u002Fspan\u003E&gt; {\\n    parents: [&amp;\u003Cspan class=\\\"hljs-symbol\\\"\u003E&#x27;a\u003C\u002Fspan\u003E Tensor&lt;\u003Cspan class=\\\"hljs-symbol\\\"\u003E&#x27;a\u003C\u002Fspan\u003E&gt;; \u003Cspan class=\\\"hljs-number\\\"\u003E2\u003C\u002Fspan\u003E],\\n}\\n\\n\u003Cspan class=\\\"hljs-comment\\\"\u003E\u002F\u002F Inside the impl block of Mul for Function\u003C\u002Fspan\u003E\\n\u003Cspan class=\\\"hljs-keyword\\\"\u003Efn\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-title function_\\\"\u003Eapply\u003C\u002Fspan\u003E(&amp;\u003Cspan class=\\\"hljs-keyword\\\"\u003Eself\u003C\u002Fspan\u003E) \u003Cspan class=\\\"hljs-punctuation\\\"\u003E-&gt;\u003C\u002Fspan\u003E Tensor {\\n    \u003Cspan class=\\\"hljs-keyword\\\"\u003Elet\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-variable\\\"\u003E_ret\u003C\u002Fspan\u003E = Tensor::\u003Cspan class=\\\"hljs-title function_ invoke__\\\"\u003Enew\u003C\u002Fspan\u003E(\u003Cspan class=\\\"hljs-keyword\\\"\u003Eself\u003C\u002Fspan\u003E.\u003Cspan class=\\\"hljs-title function_ invoke__\\\"\u003Eforward\u003C\u002Fspan\u003E(), \u003Cspan class=\\\"hljs-title function_ invoke__\\\"\u003ESome\u003C\u002Fspan\u003E(Box::\u003Cspan class=\\\"hljs-title function_ invoke__\\\"\u003Enew\u003C\u002Fspan\u003E(\u003Cspan class=\\\"hljs-keyword\\\"\u003Eself\u003C\u002Fspan\u003E)));\\n    _ret\\n} \\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ESo, \u003Ccode\u003Eapply\u003C\u002Fcode\u003E return a \u003Ccode\u003ETensor\u003C\u002Fcode\u003E with the value from the \u003Ccode\u003Eself.forward()\u003C\u002Fcode\u003E and \u003Ccode\u003E_ctx\u003C\u002Fcode\u003E equal to \u003Ccode\u003ESome(Box::new(self))\u003C\u002Fcode\u003E. The context stores the its \u003Ccode\u003Eparents\u003C\u002Fcode\u003E, this is how we know where a \u003Ccode\u003ETensor\u003C\u002Fcode\u003E come from, and this is how we can create a backward graph.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"backward\\\"\u003EBackward\u003C\u002Fh1\u003E\\n\u003Cp\u003EThe backward pass consists of the creation of a DAG graph using the final tensor, and going up the graph and supplying everyone with their \u003Ccode\u003Egrad\u003C\u002Fcode\u003E value. A filter would be good to limit whose grad to calculate,pass in the \u003Ccode\u003Ebackward\u003C\u002Fcode\u003E function,so this is left to the reader as an exercise(Calculate the grad if require_grad is \u003Ccode\u003Etrue\u003C\u002Fcode\u003E?). We \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FTopological_sorting\\\" target=\\\"_blank\\\"\u003Etopo\u003C\u002Fa\u003E sort it. \u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"hljs language-rust\\\"\u003E\u003Cspan class=\\\"hljs-keyword\\\"\u003Efn\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-title function_\\\"\u003E_deepwalk\u003C\u002Fspan\u003E(node: &amp;\u003Cspan class=\\\"hljs-symbol\\\"\u003E&#x27;a\u003C\u002Fspan\u003E Tensor&lt;\u003Cspan class=\\\"hljs-symbol\\\"\u003E&#x27;a\u003C\u002Fspan\u003E&gt;, nodes: &amp;\u003Cspan class=\\\"hljs-symbol\\\"\u003E&#x27;_\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-keyword\\\"\u003Emut\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-type\\\"\u003EVec\u003C\u002Fspan\u003E&lt;&amp;\u003Cspan class=\\\"hljs-symbol\\\"\u003E&#x27;a\u003C\u002Fspan\u003E Tensor&lt;\u003Cspan class=\\\"hljs-symbol\\\"\u003E&#x27;a\u003C\u002Fspan\u003E&gt;&gt;) {\\n    \u003Cspan class=\\\"hljs-keyword\\\"\u003Eif\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-keyword\\\"\u003Elet\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-variable\\\"\u003ESome\u003C\u002Fspan\u003E(n) = &amp;node._ctx {\\n        \u003Cspan class=\\\"hljs-keyword\\\"\u003Efor\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-variable\\\"\u003Ei\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-keyword\\\"\u003Ein\u003C\u002Fspan\u003E n.\u003Cspan class=\\\"hljs-title function_ invoke__\\\"\u003Eparents\u003C\u002Fspan\u003E() {\\n            \u003Cspan class=\\\"hljs-keyword\\\"\u003ESelf\u003C\u002Fspan\u003E::_deepwalk(i, nodes);\\n        }\\n        nodes.\u003Cspan class=\\\"hljs-title function_ invoke__\\\"\u003Epush\u003C\u002Fspan\u003E(node);\\n    }\\n}\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThis is a recursive toposorter?(Completely inspired from\u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fgeohot\u002Ftinygrad\u002F\\\" target=\\\"_blank\\\"\u003E here\u003C\u002Fa\u003E) which looks readable, unlike the one written in python(no hard feelings! I am extremely bad at reading python), and is pretty good without a filter for visited(will have to add the eventually). And, we calculate the backward pass gradients and provide it to the necessary \u003Ccode\u003ETensor\u003C\u002Fcode\u003E. I don&#39;t want this to be page to be congested with codes, so please check it out on the github repo. The loop in the \u003Ccode\u003Ebackward\u003C\u002Fcode\u003E function of the \u003Ccode\u003ETensor\u003C\u002Fcode\u003E loops in reverse topo order every tensor inside the vec returned by \u003Ccode\u003Eself.walk()\u003C\u002Fcode\u003E(Refer to github). We then apply the chain rule with every tensor getting it&#39;s grad from the tensor after(or is it before?) it. This is how the autograd process works, pretty simple; however, my explanation might have made it harder. We can expand this to use a \u003Ccode\u003Earray\u003C\u002Fcode\u003E or \u003Ccode\u003Evec\u003C\u002Fcode\u003E or arrayfire&#39;s array, which is the library I am using for my project \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Farogyad\u002FCandle\\\" target=\\\"_blank\\\"\u003ECandle\u003C\u002Fa\u003E. And finally lets look at how all this things occur. Lets write a test for a linear equation with the comments acting as the explanation. Thank you for reading!!\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"hljs language-rust\\\"\u003E\u003Cspan class=\\\"hljs-meta\\\"\u003E#[test]\u003C\u002Fspan\u003E\\n\u003Cspan class=\\\"hljs-keyword\\\"\u003Efn\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-title function_\\\"\u003Elinear_test\u003C\u002Fspan\u003E() {\\n    \u003Cspan class=\\\"hljs-comment\\\"\u003E\u002F\u002F We create two tensors\u003C\u002Fspan\u003E\\n    \u003Cspan class=\\\"hljs-keyword\\\"\u003Elet\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-variable\\\"\u003Ea\u003C\u002Fspan\u003E = Tensor::\u003Cspan class=\\\"hljs-title function_ invoke__\\\"\u003Enew\u003C\u002Fspan\u003E(\u003Cspan class=\\\"hljs-number\\\"\u003E2.0\u003C\u002Fspan\u003E, \u003Cspan class=\\\"hljs-literal\\\"\u003ENone\u003C\u002Fspan\u003E); \u003Cspan class=\\\"hljs-comment\\\"\u003E\u002F\u002F &#x27;_ctx&#x27; as None as this is formed by you and me not an operation \u003C\u002Fspan\u003E\\n    \u003Cspan class=\\\"hljs-keyword\\\"\u003Elet\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-variable\\\"\u003Eb\u003C\u002Fspan\u003E = Tensor::\u003Cspan class=\\\"hljs-title function_ invoke__\\\"\u003Enew\u003C\u002Fspan\u003E(\u003Cspan class=\\\"hljs-number\\\"\u003E3.0\u003C\u002Fspan\u003E, \u003Cspan class=\\\"hljs-literal\\\"\u003ENone\u003C\u002Fspan\u003E); \u003Cspan class=\\\"hljs-comment\\\"\u003E\u002F\u002F Same\u003C\u002Fspan\u003E\\n\\n    \u003Cspan class=\\\"hljs-comment\\\"\u003E\u002F\u002F Lets create a multiplication context which will contain tensor &#x27;a&#x27; and &#x27;b&#x27; as its parents\u003C\u002Fspan\u003E\\n    \u003Cspan class=\\\"hljs-keyword\\\"\u003Elet\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-variable\\\"\u003Ec_ctx\u003C\u002Fspan\u003E = Mul::\u003Cspan class=\\\"hljs-title function_ invoke__\\\"\u003Enew\u003C\u002Fspan\u003E([&amp;a, &amp;b]);\\n\\n    \u003Cspan class=\\\"hljs-comment\\\"\u003E\u002F\u002F Lets create the Tensor with the context &#x27;c_ctx&#x27;\u003C\u002Fspan\u003E\\n    \u003Cspan class=\\\"hljs-keyword\\\"\u003Elet\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-variable\\\"\u003Ec\u003C\u002Fspan\u003E = c_ctx.\u003Cspan class=\\\"hljs-title function_ invoke__\\\"\u003Eapply\u003C\u002Fspan\u003E();\\n\\n    \u003Cspan class=\\\"hljs-comment\\\"\u003E\u002F\u002F Let&#x27;s create the bais \u003C\u002Fspan\u003E\\n    \u003Cspan class=\\\"hljs-keyword\\\"\u003Elet\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-variable\\\"\u003Ed\u003C\u002Fspan\u003E = Tensor::\u003Cspan class=\\\"hljs-title function_ invoke__\\\"\u003Enew\u003C\u002Fspan\u003E(\u003Cspan class=\\\"hljs-number\\\"\u003E4.0\u003C\u002Fspan\u003E, \u003Cspan class=\\\"hljs-literal\\\"\u003ENone\u003C\u002Fspan\u003E); \u003Cspan class=\\\"hljs-comment\\\"\u003E\u002F\u002F Same as before the `_ctx` as None\u003C\u002Fspan\u003E\\n\\n    \u003Cspan class=\\\"hljs-comment\\\"\u003E\u002F\u002F Lets create a addition context and apply it to obtain the final output y\u003C\u002Fspan\u003E\\n    \u003Cspan class=\\\"hljs-keyword\\\"\u003Elet\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-variable\\\"\u003Ee_ctx\u003C\u002Fspan\u003E = Add::\u003Cspan class=\\\"hljs-title function_ invoke__\\\"\u003Enew\u003C\u002Fspan\u003E([&amp;c, &amp;d]);\\n    \u003Cspan class=\\\"hljs-keyword\\\"\u003Elet\u003C\u002Fspan\u003E \u003Cspan class=\\\"hljs-keyword\\\"\u003Emut \u003C\u002Fspan\u003E\u003Cspan class=\\\"hljs-variable\\\"\u003Ee\u003C\u002Fspan\u003E  = e_ctx.\u003Cspan class=\\\"hljs-title function_ invoke__\\\"\u003Eapply\u003C\u002Fspan\u003E();\\n\\n    \u003Cspan class=\\\"hljs-comment\\\"\u003E\u002F\u002F Backpropagation\u003C\u002Fspan\u003E\\n    e.\u003Cspan class=\\\"hljs-title function_ invoke__\\\"\u003Ebackward\u003C\u002Fspan\u003E();\\n\\n    \u003Cspan class=\\\"hljs-comment\\\"\u003E\u002F\u002F Will this assertion work?!\u003C\u002Fspan\u003E\\n    \u003Cspan class=\\\"hljs-built_in\\\"\u003Eassert!\u003C\u002Fspan\u003E(a.grad.\u003Cspan class=\\\"hljs-title function_ invoke__\\\"\u003Eget\u003C\u002Fspan\u003E() == \u003Cspan class=\\\"hljs-number\\\"\u003E3.0\u003C\u002Fspan\u003E); \u003Cspan class=\\\"hljs-comment\\\"\u003E\u002F\u002F The grads are inside Cell btw\u003C\u002Fspan\u003E\\n    \u003Cspan class=\\\"hljs-built_in\\\"\u003Eassert!\u003C\u002Fspan\u003E(b.grad.\u003Cspan class=\\\"hljs-title function_ invoke__\\\"\u003Eget\u003C\u002Fspan\u003E() == \u003Cspan class=\\\"hljs-number\\\"\u003E2.0\u003C\u002Fspan\u003E); \u003Cspan class=\\\"hljs-comment\\\"\u003E\u002F\u002F It was done to make the grad changing easier\u003C\u002Fspan\u003E\\n} \\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\",\"bodyBegin\":6,\"frontmatter\":\"title: Autograd in Rust\\ndate: Aug 12, 2021\"},\"prev\":{\"link\":\"2021-07-15-polynomial\",\"blog\":{\"date\":\"2021-07-15\",\"title\":\"Polynomial\"}}}}"}</script>
		</div>
	</body>
</html>
