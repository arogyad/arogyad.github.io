{"content":{"out":{"attributes":{"title":"Autograd in Rust","date":"2021-08-12"},"body":"<h1 id=\"introduction\">Introduction</h1>\n<p>In my quest for learning <em><strong>Rust</strong></em>, I had started writing a machine learning library. Machine learning is pretty interesting , but deep learning takes the cake when it comes to the weirdness. One main thing that is required for a deep learning library is a auto differentiation engine. An autodiff engine is a piece of code that sits on top and creates a graph for how the backward propagation should be done. I wanted a autograd library for my project so tried writing it. I was more interested in dynamic graphsish autograd style something like that of <a href=\"https://pytorch.org/\" target='_blank'> pytorch </a> rather than the static graph of tensorflow. Pytorch specifically creates a <a href=\"https://en.wikipedia.org/wiki/Directed_acyclic_graph\" target='_blank'>DAG graph</a>. The reverse topilogically sorted Directed Graph (DAG) is ,then, used to create a graph for backward propagation. <a href=\"https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\" target=\"_blank\">Here</a> is a link for further reading on the autograd of pytorch. This blog is quite long as there is a lot to talk about.</p>\n<h1 id=\"background\">Background</h1>\n<p>The implementation is heavly inspired from <a href=\"https://github.com/geohot/tinygrad/\" target=\"_blank\"> tinygrad </a>  and pytorch. The implementation shown here is an extremely small part, and a great deal of other fields and methods are yet to be added. Here, we will be creating a autograd library in rust. We will be using <code>f64</code> values, but we can easily replace that with anything else. The use of <code>f64</code> here is just for easier explanation. One difference when compared to the tinygrad&#39;s implementation is that, due to the nature of no garbage collection language, we don&#39;t need to store the <code>parents</code> and the <code>saved_tensor</code> in two objects, but we will get to that point in the future.</p>\n<h1 id=\"tensor-class\">&quot;Tensor&quot; Class</h1>\n<p>It gives me immense guilt to call the &quot;Tensor&quot; struct here as the &quot;Tensor&quot; as the bar for something to be called a tensor is too high. It is nothing but the absolute minimum that we require to create and show a autograd engine. The &quot;Tensor&quot; struct is defined as such:</p>\n<pre><code class=\"hljs language-rust\"><span class=\"hljs-comment\">// /src/tensor.rs/Tensor</span>\n<span class=\"hljs-keyword\">pub</span> <span class=\"hljs-keyword\">struct</span> <span class=\"hljs-title class_\">Tensor</span>&lt;<span class=\"hljs-symbol\">&#x27;a</span>&gt; {\n    <span class=\"hljs-keyword\">pub</span> data: <span class=\"hljs-type\">f64</span>,\n    <span class=\"hljs-keyword\">pub</span> grad: Cell&lt;<span class=\"hljs-type\">f64</span>&gt;,\n    <span class=\"hljs-keyword\">pub</span> _ctx: <span class=\"hljs-type\">Option</span>&lt;<span class=\"hljs-type\">Box</span>&lt;&amp;<span class=\"hljs-symbol\">&#x27;a</span> <span class=\"hljs-keyword\">dyn</span> Function&gt;&gt;,\n}\n</code></pre>\n<p>This isn&#39;t much but this does the work done, atleast for the explanation.  The <code>_ctx</code> is the context field, which stores the process of creation of the <code>self</code> tensor, other are self explanatory. You might be wondering why the <code>_ctx</code> is a <code>Box&lt;&amp;&#39; dyn Function&gt;</code> and not a generic-it is easier to read and simpler this way, using generics will increase the complexity without much gain (also I don&#39;t know if will generics work). The <code>Tensor</code> declares a <code>new</code> static function which is self explanatory. You can check out the code on my <a href=\"https://github.com/arogyad/autograd\" target=\"_blank\">github</a>. We will talk about other functions as we go along the way.</p>\n<h2 id=\"where-does-the-_ctx-come-from-\">Where does the &quot;_ctx&quot; come from ?!</h2>\n<p>The <code>_ctx</code> is an object which implements the <code>Function</code> trait. The function trait is defined as such</p>\n<pre><code class=\"hljs language-rust\"><span class=\"hljs-comment\">// /src/ftrait.rs/Function</span>\n<span class=\"hljs-keyword\">pub</span> <span class=\"hljs-keyword\">trait</span> <span class=\"hljs-title class_\">Function</span> {\n    <span class=\"hljs-keyword\">fn</span> <span class=\"hljs-title function_\">apply</span>(&amp;<span class=\"hljs-keyword\">self</span>) <span class=\"hljs-punctuation\">-&gt;</span> Tensor;\n    <span class=\"hljs-keyword\">fn</span> <span class=\"hljs-title function_\">backward</span>(&amp;<span class=\"hljs-keyword\">self</span>, grad: <span class=\"hljs-type\">f64</span>) <span class=\"hljs-punctuation\">-&gt;</span> [<span class=\"hljs-type\">f64</span>; <span class=\"hljs-number\">2</span>];\n    <span class=\"hljs-keyword\">fn</span> <span class=\"hljs-title function_\">forward</span>(&amp;<span class=\"hljs-keyword\">self</span>) <span class=\"hljs-punctuation\">-&gt;</span> <span class=\"hljs-type\">f64</span>;\n    <span class=\"hljs-keyword\">fn</span> <span class=\"hljs-title function_\">parents</span>(&amp;<span class=\"hljs-keyword\">self</span>) <span class=\"hljs-punctuation\">-&gt;</span> [&amp;Tensor; <span class=\"hljs-number\">2</span>];\n}\n</code></pre>\n<p>The <code>parents</code> function returns the parent inside of the struct, which implements the <code>Function</code> trait. This is important for the creation of graph during backward propagation. The function trait here is defined for binary operations, but we could have made it for other operations as well by using maybe a const generic type( <code>&lt;const X: i32&gt;</code> )?! <code>Apply</code> performs the <code>forward</code> function and returns the required tensor containing its own context(We will see this later). The <code>backward</code> function performs the backward computation,i.e it calculates the grad based on the incoming <code>grad</code>(chain rule). We will implement the <code>Function</code> trait for multiplication type. We won&#39;t be overriding any operator functions such as <code>*</code> operator. This will show us the bare metal, how things are working. Lets look at the implementation of <code>Mul</code>. <code>Forward</code> and <code>Backward</code> are trivial so we will be looking at the <code>apply</code> function&#39;s implementation. <code>Mul</code> struct is defined as</p>\n<pre><code class=\"hljs language-rust\"><span class=\"hljs-comment\">// /src/Function.rs/Mul</span>\n<span class=\"hljs-keyword\">pub</span> <span class=\"hljs-keyword\">struct</span> <span class=\"hljs-title class_\">Mul</span>&lt;<span class=\"hljs-symbol\">&#x27;a</span>&gt; {\n    parents: [&amp;<span class=\"hljs-symbol\">&#x27;a</span> Tensor&lt;<span class=\"hljs-symbol\">&#x27;a</span>&gt;; <span class=\"hljs-number\">2</span>],\n}\n\n<span class=\"hljs-comment\">// Inside the impl block of Mul for Function</span>\n<span class=\"hljs-keyword\">fn</span> <span class=\"hljs-title function_\">apply</span>(&amp;<span class=\"hljs-keyword\">self</span>) <span class=\"hljs-punctuation\">-&gt;</span> Tensor {\n    <span class=\"hljs-keyword\">let</span> <span class=\"hljs-variable\">_ret</span> = Tensor::<span class=\"hljs-title function_ invoke__\">new</span>(<span class=\"hljs-keyword\">self</span>.<span class=\"hljs-title function_ invoke__\">forward</span>(), <span class=\"hljs-title function_ invoke__\">Some</span>(Box::<span class=\"hljs-title function_ invoke__\">new</span>(<span class=\"hljs-keyword\">self</span>)));\n    _ret\n} \n</code></pre>\n<p>So, <code>apply</code> return a <code>Tensor</code> with the value from the <code>self.forward()</code> and <code>_ctx</code> equal to <code>Some(Box::new(self))</code>. The context stores the its <code>parents</code>, this is how we know where a <code>Tensor</code> come from, and this is how we can create a backward graph.</p>\n<h1 id=\"backward\">Backward</h1>\n<p>The backward pass consists of the creation of a DAG graph using the final tensor, and going up the graph and supplying everyone with their <code>grad</code> value. A filter would be good to limit whose grad to calculate,pass in the <code>backward</code> function,so this is left to the reader as an exercise(Calculate the grad if require_grad is <code>true</code>?). We <a href=\"https://en.wikipedia.org/wiki/Topological_sorting\" target=\"_blank\">topo</a> sort it. </p>\n<pre><code class=\"hljs language-rust\"><span class=\"hljs-keyword\">fn</span> <span class=\"hljs-title function_\">_deepwalk</span>(node: &amp;<span class=\"hljs-symbol\">&#x27;a</span> Tensor&lt;<span class=\"hljs-symbol\">&#x27;a</span>&gt;, nodes: &amp;<span class=\"hljs-symbol\">&#x27;_</span> <span class=\"hljs-keyword\">mut</span> <span class=\"hljs-type\">Vec</span>&lt;&amp;<span class=\"hljs-symbol\">&#x27;a</span> Tensor&lt;<span class=\"hljs-symbol\">&#x27;a</span>&gt;&gt;) {\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">let</span> <span class=\"hljs-variable\">Some</span>(n) = &amp;node._ctx {\n        <span class=\"hljs-keyword\">for</span> <span class=\"hljs-variable\">i</span> <span class=\"hljs-keyword\">in</span> n.<span class=\"hljs-title function_ invoke__\">parents</span>() {\n            <span class=\"hljs-keyword\">Self</span>::_deepwalk(i, nodes);\n        }\n        nodes.<span class=\"hljs-title function_ invoke__\">push</span>(node);\n    }\n}\n</code></pre>\n<p>This is a recursive toposorter?(Completely inspired from<a href=\"https://github.com/geohot/tinygrad/\" target=\"_blank\"> here</a>) which looks readable, unlike the one written in python(no hard feelings! I am extremely bad at reading python), and is pretty good without a filter for visited(will have to add the eventually). And, we calculate the backward pass gradients and provide it to the necessary <code>Tensor</code>. I don&#39;t want this to be page to be congested with codes, so please check it out on the github repo. The loop in the <code>backward</code> function of the <code>Tensor</code> loops in reverse topo order every tensor inside the vec returned by <code>self.walk()</code>(Refer to github). We then apply the chain rule with every tensor getting it&#39;s grad from the tensor after(or is it before?) it. This is how the autograd process works, pretty simple; however, my explanation might have made it harder. We can expand this to use a <code>array</code> or <code>vec</code> or arrayfire&#39;s array, which is the library I am using for my project <a href=\"https://github.com/arogyad/Candle\" target=\"_blank\">Candle</a>. And finally lets look at how all this things occur. Lets write a test for a linear equation with the comments acting as the explanation. Thank you for reading!!</p>\n<pre><code class=\"hljs language-rust\"><span class=\"hljs-meta\">#[test]</span>\n<span class=\"hljs-keyword\">fn</span> <span class=\"hljs-title function_\">linear_test</span>() {\n    <span class=\"hljs-comment\">// We create two tensors</span>\n    <span class=\"hljs-keyword\">let</span> <span class=\"hljs-variable\">a</span> = Tensor::<span class=\"hljs-title function_ invoke__\">new</span>(<span class=\"hljs-number\">2.0</span>, <span class=\"hljs-literal\">None</span>); <span class=\"hljs-comment\">// &#x27;_ctx&#x27; as None as this is formed by you and me not an operation </span>\n    <span class=\"hljs-keyword\">let</span> <span class=\"hljs-variable\">b</span> = Tensor::<span class=\"hljs-title function_ invoke__\">new</span>(<span class=\"hljs-number\">3.0</span>, <span class=\"hljs-literal\">None</span>); <span class=\"hljs-comment\">// Same</span>\n\n    <span class=\"hljs-comment\">// Lets create a multiplication context which will contain tensor &#x27;a&#x27; and &#x27;b&#x27; as its parents</span>\n    <span class=\"hljs-keyword\">let</span> <span class=\"hljs-variable\">c_ctx</span> = Mul::<span class=\"hljs-title function_ invoke__\">new</span>([&amp;a, &amp;b]);\n\n    <span class=\"hljs-comment\">// Lets create the Tensor with the context &#x27;c_ctx&#x27;</span>\n    <span class=\"hljs-keyword\">let</span> <span class=\"hljs-variable\">c</span> = c_ctx.<span class=\"hljs-title function_ invoke__\">apply</span>();\n\n    <span class=\"hljs-comment\">// Let&#x27;s create the bais </span>\n    <span class=\"hljs-keyword\">let</span> <span class=\"hljs-variable\">d</span> = Tensor::<span class=\"hljs-title function_ invoke__\">new</span>(<span class=\"hljs-number\">4.0</span>, <span class=\"hljs-literal\">None</span>); <span class=\"hljs-comment\">// Same as before the `_ctx` as None</span>\n\n    <span class=\"hljs-comment\">// Lets create a addition context and apply it to obtain the final output y</span>\n    <span class=\"hljs-keyword\">let</span> <span class=\"hljs-variable\">e_ctx</span> = Add::<span class=\"hljs-title function_ invoke__\">new</span>([&amp;c, &amp;d]);\n    <span class=\"hljs-keyword\">let</span> <span class=\"hljs-keyword\">mut </span><span class=\"hljs-variable\">e</span>  = e_ctx.<span class=\"hljs-title function_ invoke__\">apply</span>();\n\n    <span class=\"hljs-comment\">// Backpropagation</span>\n    e.<span class=\"hljs-title function_ invoke__\">backward</span>();\n\n    <span class=\"hljs-comment\">// Will this assertion work?!</span>\n    <span class=\"hljs-built_in\">assert!</span>(a.grad.<span class=\"hljs-title function_ invoke__\">get</span>() == <span class=\"hljs-number\">3.0</span>); <span class=\"hljs-comment\">// The grads are inside Cell btw</span>\n    <span class=\"hljs-built_in\">assert!</span>(b.grad.<span class=\"hljs-title function_ invoke__\">get</span>() == <span class=\"hljs-number\">2.0</span>); <span class=\"hljs-comment\">// It was done to make the grad changing easier</span>\n} \n</code></pre>\n","bodyBegin":6,"frontmatter":"title: Autograd in Rust\ndate: Aug 12, 2021"},"prev":{"link":"2021-07-15-polynomial","blog":{"date":"2021-07-15","title":"Polynomial"}}}}